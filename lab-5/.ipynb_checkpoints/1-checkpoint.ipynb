{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266bcdb-71ce-496e-abe0-d74b3b16bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to perform the following operations and show the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30900fd4-de76-47b9-956a-21860dfc134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.\tPre-process the data removing URL, hashtag, URL, punctuation, user annotation, stopwords.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer  # Consider lemmatization if context matters\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Preprocesses text by removing URLs, hashtags, punctuation, user annotations,\n",
    "  and stop words, and performs stemming (consider lemmatization for context-aware).\n",
    "\n",
    "  Args:\n",
    "      text: The text to preprocess.\n",
    "\n",
    "  Returns:\n",
    "      A list of preprocessed words (lowercase, no URLs, hashtags, punctuation,\n",
    "      user annotations, or stop words, stemmed).\n",
    "  \"\"\"\n",
    "\n",
    "  # Lowercase all words\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove URLs using regular expression\n",
    "  text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "  # Remove hashtags (consider using dedicated hashtag detection if needed)\n",
    "  text = re.sub(r\"#\\S+\", \"\", text)\n",
    "\n",
    "  # Remove punctuation\n",
    "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "  # Remove user annotations (consider a more specific pattern for your use case)\n",
    "  text = re.sub(r\"@\\S+\", \"\", text)\n",
    "\n",
    "  # Remove stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "  # Stem words using PorterStemmer (consider lemmatization for context preservation)\n",
    "  stemmer = PorterStemmer()\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "  return stemmed_words\n",
    "\n",
    "def create_processed_file(input_file, output_file):\n",
    "  \"\"\"Creates a new file with preprocessed text from the input file.\n",
    "\n",
    "  Args:\n",
    "      input_file: Path to the input file.\n",
    "      output_file: Path to the output file.\n",
    "  \"\"\"\n",
    "\n",
    "  with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "    for line in f_in:\n",
    "      processed_line = preprocess_text(line.strip())  # Remove trailing newline\n",
    "      f_out.write(\" \".join(processed_line) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  nltk.download('punkt', quiet=True)  # Download NLTK punkt tokenizer (if not already downloaded)\n",
    "  nltk.download('stopwords', quiet=True)  # Download NLTK stopwords corpus (if not already downloaded)\n",
    "  input_file = \"your_input_file.txt\"  # Replace with your actual file path\n",
    "  output_file = \"processed_text.txt\"\n",
    "  create_processed_file(input_file, output_file)\n",
    "  print(f\"Preprocessed text written to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f428895-f222-409f-a49e-687f9a1cb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. Apply the method stemming.\n",
    "\n",
    "  # Remove stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "  # Stem words using PorterStemmer (consider lemmatization for context preservation)\n",
    "  stemmer = PorterStemmer()\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae131a6-e0d2-4401-9a96-c9bd8da9e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. Represent the text data into vector format.\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses text by removing URLs, hashtags, punctuation, user annotations,\n",
    "    and stop words, and performs stemming.\n",
    "\n",
    "    Args:\n",
    "        text: The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        A list of preprocessed words (lowercase, no URLs, hashtags, punctuation,\n",
    "        user annotations, or stop words, stemmed).\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase all words\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs using regular expression\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Remove hashtags (consider using dedicated hashtag detection if needed)\n",
    "    text = re.sub(r\"#\\S+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove user annotations (consider a more specific pattern for your use case)\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "    # Stem words using PorterStemmer (consider lemmatization for context preservation)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "def create_tfidf_vectorizer(documents):\n",
    "    \"\"\"Creates a TF-IDF vectorizer from a list of preprocessed documents.\n",
    "\n",
    "    Args:\n",
    "        documents: A list of preprocessed documents (lists of words).\n",
    "\n",
    "    Returns:\n",
    "        A TF-IDF vectorizer object.\n",
    "    \"\"\"\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, min_df=2)  # Adjust parameters as needed\n",
    "    vectorizer.fit_transform(documents)\n",
    "    return vectorizer\n",
    "\n",
    "def main():\n",
    "    nltk.download('punkt', quiet=True)  # Download NLTK punkt tokenizer (if not already downloaded)\n",
    "    nltk.download('stopwords', quiet=True)  # Download NLTK stopwords corpus (if not already downloaded)\n",
    "\n",
    "    # Example documents (replace with your actual data)\n",
    "    documents = [\n",
    "        \"This is the first document with some words about data science.\",\n",
    "        \"This is the second document with some overlapping words but focuses on machine learning.\",\n",
    "        \"This document has some unique words related to natural language processing.\"\n",
    "    ]\n",
    "\n",
    "    # Preprocess documents\n",
    "    preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = create_tfidf_vectorizer(preprocessed_documents)\n",
    "\n",
    "    # Get the vocabulary (feature names)\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Transform documents into TF-IDF vectors (consider using pandas for a better representation)\n",
    "    tfidf_matrix = vectorizer.transform(preprocessed_documents)\n",
    "\n",
    "    print(\"TF-IDF Matrix:\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        for word, score in zip(vocabulary, tfidf_matrix[i].toarray()[0]):\n",
    "            print(f\"\\t- {word}: {score:.4f}\")  # Format score with 4 decimal places\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca26c07-4c38-4b88-a2b7-a83cdf408a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d.\tCreate a directed graph considering each data row as nodes of the graph.\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.similarity.cosine import cosine_similarity\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Preprocesses text by removing stop words, performing stemming, and tokenization.\n",
    "\n",
    "  Args:\n",
    "      text: The text to preprocess.\n",
    "\n",
    "  Returns:\n",
    "      A list of preprocessed words (lowercase, no stop words, stemmed, and tokenized).\n",
    "  \"\"\"\n",
    "\n",
    "  # Lowercase all words\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = [word for word in word_tokenize(text) if word not in stop_words]\n",
    "\n",
    "  # Stem words using PorterStemmer (consider lemmatization for context preservation)\n",
    "  stemmer = PorterStemmer()\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "  return stemmed_words\n",
    "\n",
    "def create_directed_graph(data, similarity_threshold=0.5):\n",
    "  \"\"\"\n",
    "  Creates a directed graph considering each data row as a node and linking\n",
    "  similar rows based on text similarity using cosine similarity.\n",
    "\n",
    "  Args:\n",
    "      data: A list of data rows (strings).\n",
    "      similarity_threshold: The minimum cosine similarity score for a connection (default 0.5).\n",
    "\n",
    "  Returns:\n",
    "      A dictionary representing the directed graph, where keys are data rows (strings)\n",
    "      and values are lists of similar data rows (strings).\n",
    "  \"\"\"\n",
    "\n",
    "  graph = defaultdict(list)\n",
    "  preprocessed_data = [preprocess_text(row) for row in data]\n",
    "\n",
    "  for i, row1 in enumerate(data):\n",
    "    for j, row2 in enumerate(data):\n",
    "      if i != j:  # Avoid self-connections\n",
    "        similarity = cosine_similarity(preprocessed_data[i], preprocessed_data[j])\n",
    "        if similarity >= similarity_threshold:\n",
    "          graph[row1].append(row2)\n",
    "\n",
    "  return graph\n",
    "\n",
    "def main():\n",
    "  # Example data (replace with your actual data)\n",
    "  data = [\n",
    "      \"This is a document about data science concepts.\",\n",
    "      \"Machine learning algorithms are used in data science.\",\n",
    "      \"Natural language processing is another field related to data science.\",\n",
    "      \"This document talks about applications of data science in healthcare.\",\n",
    "  ]\n",
    "\n",
    "  # Create directed graph (adjust similarity_threshold as needed)\n",
    "  graph = create_directed_graph(data, similarity_threshold=0.6)\n",
    "\n",
    "  print(\"Directed Graph:\")\n",
    "  for node, neighbors in graph.items():\n",
    "    print(f\"{node}: {', '.join(neighbors)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e0cd6-8f31-4855-8a71-67983f025ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#e.\tCreate edge between any two nodes if data rows have more than 1 common words at least.\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Preprocesses text by removing stop words, performing stemming, and tokenization.\n",
    "\n",
    "  Args:\n",
    "      text: The text to preprocess.\n",
    "\n",
    "  Returns:\n",
    "      A set of preprocessed words (lowercase, no stop words, stemmed).\n",
    "  \"\"\"\n",
    "\n",
    "  # Lowercase all words\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = [word for word in word_tokenize(text) if word not in stop_words]\n",
    "\n",
    "  # Stem words using PorterStemmer (consider lemmatization for context preservation)\n",
    "  stemmer = PorterStemmer()\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "  return set(stemmed_words)  # Return set for efficient common word counting\n",
    "\n",
    "def create_directed_graph(data, min_common_words=2):\n",
    "  \"\"\"\n",
    "  Creates a directed graph considering each data row as a node and linking\n",
    "  similar rows based on the number of common words (at least a certain threshold).\n",
    "\n",
    "  Args:\n",
    "      data: A list of data rows (strings).\n",
    "      min_common_words: The minimum number of common words for a connection (default 2).\n",
    "\n",
    "  Returns:\n",
    "      A dictionary representing the directed graph, where keys are data rows (strings)\n",
    "      and values are lists of similar data rows (strings).\n",
    "  \"\"\"\n",
    "\n",
    "  graph = defaultdict(list)\n",
    "  preprocessed_data = [preprocess_text(row) for row in data]\n",
    "\n",
    "  for i, row1 in enumerate(data):\n",
    "    for j, row2 in enumerate(data):\n",
    "      if i != j:  # Avoid self-connections\n",
    "        common_words = len(preprocessed_data[i] & preprocessed_data[j])\n",
    "        if common_words >= min_common_words:\n",
    "          graph[row1].append(row2)\n",
    "\n",
    "  return graph\n",
    "\n",
    "def main():\n",
    "  # Example data (replace with your actual data)\n",
    "  data = [\n",
    "      \"This is a document about data science concepts.\",\n",
    "      \"Machine learning algorithms are used in data science.\",\n",
    "      \"Natural language processing is another field related to data science.\",\n",
    "      \"This document talks about applications of data science in healthcare.\",\n",
    "      \"Another example document about data science and machine learning.\"\n",
    "  ]\n",
    "\n",
    "  # Create directed graph (adjust min_common_words as needed)\n",
    "  graph = create_directed_graph(data, min_common_words=2)\n",
    "\n",
    "  print(\"Directed Graph:\")\n",
    "  for node, neighbors in graph.items():\n",
    "    print(f\"{node}: {', '.join(neighbors)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e61abd-11f7-46de-aeca-914285259456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f. Compute the weight of the edges between the two nodes considering the distances(Euclidean /manhattan distance)/similarity(cosine similarity) between every pare of node.\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.similarity.cosine import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean, manhattan\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Preprocesses text by removing stop words, performing stemming, and tokenization.\n",
    "\n",
    "  Args:\n",
    "      text: The text to preprocess.\n",
    "\n",
    "  Returns:\n",
    "      A list of preprocessed words (lowercase, no stop words, stemmed).\n",
    "  \"\"\"\n",
    "\n",
    "  # Lowercase all words\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = [word for word in word_tokenize(text) if word not in stop_words]\n",
    "\n",
    "  # Stem words using PorterStemmer (consider lemmatization for context preservation)\n",
    "  stemmer = PorterStemmer()\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "  return stemmed_words\n",
    "\n",
    "def create_weighted_graph(data, distance_metric=\"cosine\"):\n",
    "  \"\"\"\n",
    "  Creates a weighted directed graph considering each data row as a node and linking\n",
    "  similar rows based on cosine similarity. The weight of the edge is based on the\n",
    "  chosen distance metric.\n",
    "\n",
    "  Args:\n",
    "      data: A list of data rows (strings).\n",
    "      distance_metric: The distance metric to use for weight calculation (\"cosine\",\n",
    "                        \"euclidean\", or \"manhattan\"). Default is \"cosine\".\n",
    "\n",
    "  Returns:\n",
    "      A dictionary representing the weighted directed graph, where keys are data rows\n",
    "      (strings) and values are dictionaries of connected nodes (strings) and their weights.\n",
    "  \"\"\"\n",
    "\n",
    "  graph = defaultdict(dict)\n",
    "  preprocessed_data = [preprocess_text(row) for row in data]\n",
    "\n",
    "  for i, row1 in enumerate(data):\n",
    "    for j, row2 in enumerate(data):\n",
    "      if i != j:  # Avoid self-connections\n",
    "        # Calculate similarity or distance based on the chosen metric\n",
    "        if distance_metric == \"cosine\":\n",
    "          similarity = cosine_similarity(preprocessed_data[i], preprocessed_data[j])\n",
    "          weight = 1 - similarity  # Higher similarity, lower weight (closer)\n",
    "        elif distance_metric == \"euclidean\":\n",
    "          weight = euclidean(preprocessed_data[i], preprocessed_data[j])\n",
    "        elif distance_metric == \"manhattan\":\n",
    "          weight = manhattan(preprocessed_data[i], preprocessed_data[j])\n",
    "        else:\n",
    "          raise ValueError(f\"Invalid distance metric: {distance_metric}\")\n",
    "\n",
    "        graph[row1][row2] = weight\n",
    "\n",
    "  return graph\n",
    "\n",
    "def main():\n",
    "  # Example data (replace with your actual data)\n",
    "  data = [\n",
    "      \"This is a document about data science concepts.\",\n",
    "      \"Machine learning algorithms are used in data science.\",\n",
    "      \"Natural language processing is another field related to data science.\",\n",
    "      \"This document talks about applications of data science in healthcare.\",\n",
    "      \"Another example document about data science and machine learning.\"\n",
    "  ]\n",
    "\n",
    "  # Create weighted directed graph (adjust distance_metric as needed)\n",
    "  graph = create_weighted_graph(data, distance_metric=\"cosine\")\n",
    "\n",
    "  print(\"Weighted Directed Graph:\")\n",
    "  for node, neighbors in graph.items():\n",
    "    print(f\"{node}:\")\n",
    "    for neighbor, weight in neighbors.items():\n",
    "      print(f\"\\t- {neighbor}: {weight:.4f}\")  # Format weight with 4 decimal places\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
