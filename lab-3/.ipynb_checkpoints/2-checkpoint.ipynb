{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d99745-689f-4566-978b-a5a6ce9a084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to perform the following operations and show the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4196dcb-a744-48f8-9ef8-71583b40fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Create a file with where each line consists of stemmed version of words(lowercase) without stop words, URL, punctuation.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Preprocesses text by removing stop words, URLs, punctuation, and stemming words.\n",
    "\n",
    "  Args:\n",
    "      text: The text to preprocess.\n",
    "\n",
    "  Returns:\n",
    "      A list of stemmed words (lowercase) without stop words, URLs, or punctuation.\n",
    "  \"\"\"\n",
    "  # Lowercase all words\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove URLs using regular expression\n",
    "  text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "  # Remove punctuation\n",
    "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "  # Remove stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "  # Stem words using Porter stemmer\n",
    "  stemmer = PorterStemmer()\n",
    "  stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "  return stemmed_words\n",
    "\n",
    "def create_processed_file(input_file, output_file):\n",
    "  \"\"\"Creates a new file with preprocessed text from the input file.\n",
    "\n",
    "  Args:\n",
    "      input_file: Path to the input file.\n",
    "      output_file: Path to the output file.\n",
    "  \"\"\"\n",
    "  with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "    for line in f_in:\n",
    "      processed_line = preprocess_text(line.strip())  # Remove trailing newline\n",
    "      f_out.write(\" \".join(processed_line) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  input_file = \"your_input_file.txt\"  # Replace with your actual file path\n",
    "  output_file = \"processed_text.txt\"\n",
    "  create_processed_file(input_file, output_file)\n",
    "  print(f\"Preprocessed text written to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d12add-62c0-4163-985c-710a8ef59ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find synonyms of each distinct words.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses text by removing stop words, URLs, punctuation,\n",
    "    and performing lemmatization.\n",
    "\n",
    "    Args:\n",
    "        text: The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        A list of preprocessed words (lowercase, no stop words, URLs,\n",
    "        or punctuation, and lemmatized).\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase all words\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs using regular expression\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "    # Lemmatize words using WordNetLemmatizer\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stemmed_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "def find_synonyms(preprocessed_words):\n",
    "    \"\"\"Finds synonyms for each distinct word in the preprocessed list.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_words: A list of preprocessed words.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are distinct preprocessed words and\n",
    "        values are lists of their synonyms.\n",
    "    \"\"\"\n",
    "\n",
    "    synonyms_dict = {}\n",
    "    for word in set(preprocessed_words):\n",
    "        synonyms = []\n",
    "        synsets = wordnet.synsets(word)  # Get synsets for the word\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                synonym = lemma.name()\n",
    "                if synonym != word and synonym not in synonyms:  # Avoid duplicates\n",
    "                    synonyms.append(synonym)\n",
    "        synonyms_dict[word] = synonyms\n",
    "\n",
    "    return synonyms_dict\n",
    "\n",
    "def main():\n",
    "    nltk.download('punkt', quiet=True)  # Download NLTK punkt tokenizer (if not already downloaded)\n",
    "    nltk.download('wordnet', quiet=True)  # Download NLTK WordNet (if not already downloaded)\n",
    "    input_file = \"your_input_file.txt\"  # Replace with your actual file path\n",
    "    output_file = \"processed_text.txt\"\n",
    "\n",
    "    with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:\n",
    "        for line in f_in:\n",
    "            processed_line = preprocess_text(line.strip())\n",
    "            f_out.write(\" \".join(processed_line) + \"\\n\")\n",
    "\n",
    "    # Read preprocessed text from output file (optional, for demonstration)\n",
    "    with open(output_file, 'r') as f:\n",
    "        preprocessed_text = f.read().splitlines()\n",
    "\n",
    "    # Find synonyms for each distinct word\n",
    "    synonyms_dict = find_synonyms(preprocessed_text)\n",
    "\n",
    "    print(\"Original words and their synonyms:\")\n",
    "    for word, synonyms in synonyms_dict.items():\n",
    "        print(f\"{word}: {', '.join(synonyms)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c85e3-f0db-4e33-b331-a6b7e6ebd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the frequency of each hash tag. Consider a dictionary to store the hashtags and its frequency. Sort the dictionary based on frequency values. Display highest occurred hashtag.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def find_hashtag_frequencies(text):\n",
    "  \"\"\"\n",
    "  Finds the frequency of each hashtag in a given text.\n",
    "\n",
    "  Args:\n",
    "      text: The text containing hashtags.\n",
    "\n",
    "  Returns:\n",
    "      A dictionary where keys are hashtags (lowercase, without #) and\n",
    "      values are their frequencies.\n",
    "  \"\"\"\n",
    "  hashtags = []\n",
    "  # Find all words starting with '#' (case-insensitive)\n",
    "  for word in re.findall(r\"#(\\w+)\", text, flags=re.IGNORECASE):\n",
    "    hashtags.append(word.lower())  # Convert to lowercase\n",
    "\n",
    "  # Count hashtag frequencies using Counter\n",
    "  hashtag_counts = Counter(hashtags)\n",
    "\n",
    "  return hashtag_counts\n",
    "\n",
    "def main():\n",
    "  # Example text (replace with your actual text)\n",
    "  text = \"\"\"This is a tweet with #SomeHashtags and #moreHashtags. \n",
    "  #AnotherHashtag is here and so is #ThisOne. Let's see what the most popular one is!\"\"\"\n",
    "\n",
    "  # Find hashtag frequencies\n",
    "  hashtag_counts = find_hashtag_frequencies(text)\n",
    "\n",
    "  # Sort hashtags by frequency (descending order)\n",
    "  sorted_hashtags = sorted(hashtag_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "  # Display the highest occurred hashtag and its frequency\n",
    "  if sorted_hashtags:\n",
    "    most_popular_hashtag, frequency = sorted_hashtags[0]\n",
    "    print(f\"Most popular hashtag: #{most_popular_hashtag} (used {frequency} times)\")\n",
    "  else:\n",
    "    print(\"No hashtags found in the text.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4c25d-2775-479a-8d79-434c51452298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify duplicate strings.\n",
    "\n",
    "def find_duplicate_strings(strings):\n",
    "  \"\"\"\n",
    "  Finds duplicate strings in a list using a set.\n",
    "\n",
    "  Args:\n",
    "      strings: A list of strings.\n",
    "\n",
    "  Returns:\n",
    "      A list of duplicate strings.\n",
    "  \"\"\"\n",
    "  seen_strings = set()\n",
    "  duplicates = []\n",
    "  for string in strings:\n",
    "    if string in seen_strings:\n",
    "      duplicates.append(string)\n",
    "    else:\n",
    "      seen_strings.add(string)\n",
    "  return duplicates\n",
    "\n",
    "# Example list of strings\n",
    "strings = [\"apple\", \"banana\", \"cherry\", \"apple\", \"orange\", \"banana\"]\n",
    "\n",
    "# Find duplicate strings\n",
    "duplicates = find_duplicate_strings(strings)\n",
    "\n",
    "if duplicates:\n",
    "  print(\"Duplicate strings:\", \", \".join(duplicates))\n",
    "else:\n",
    "  print(\"No duplicate strings found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d899fb2-f402-4b7f-830f-afd459c43796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Represent the document as Text Vectorization considering word presence as 1 and absence as 0. Each row represents the row of the file and column represents all the distinct words\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def create_document_term_matrix(documents):\n",
    "  \"\"\"\n",
    "  Creates a document-term matrix representing word presence in documents.\n",
    "\n",
    "  Args:\n",
    "      documents: A list of documents (strings).\n",
    "\n",
    "  Returns:\n",
    "      A dictionary where keys are words and values are lists representing\n",
    "      word presence (1) or absence (0) in each document.\n",
    "  \"\"\"\n",
    "\n",
    "  # Preprocess documents (remove stop words, lowercase)\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  processed_docs = [[word.lower() for word in doc.split() if word not in stop_words] for doc in documents]\n",
    "\n",
    "  # Create a vocabulary of distinct words\n",
    "  vocabulary = set()\n",
    "  for doc in processed_docs:\n",
    "    vocabulary.update(doc)\n",
    "\n",
    "  # Create the document-term matrix\n",
    "  document_term_matrix = {}\n",
    "  for word in vocabulary:\n",
    "    document_term_matrix[word] = []\n",
    "    for doc in processed_docs:\n",
    "      document_term_matrix[word].append(1 if word in doc else 0)\n",
    "\n",
    "  return document_term_matrix\n",
    "\n",
    "def main():\n",
    "  # Example documents (replace with your actual data)\n",
    "  documents = [\n",
    "      \"This is the first document with some words.\",\n",
    "      \"This is the second document with some overlapping words.\",\n",
    "      \"This document has some unique words as well.\"\n",
    "  ]\n",
    "\n",
    "  # Create document-term matrix\n",
    "  document_term_matrix = create_document_term_matrix(documents)\n",
    "\n",
    "  # Print the matrix (consider using pandas for a better representation)\n",
    "  print(\"Document-Term Matrix (word presence: 1, absence: 0):\")\n",
    "  for word, presence_list in document_term_matrix.items():\n",
    "    print(f\"{word}: {presence_list}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
